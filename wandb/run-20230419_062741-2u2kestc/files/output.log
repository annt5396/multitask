
  0%|                                                                                                                                                                                                             | 0/1840 [00:00<?, ?it/s]You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.








































































































































































 20%|███████████████████████████████████████                                                                                                                                                            | 368/1840 [05:44<20:10,  1.22it/s]































100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 257/257 [01:02<00:00,  4.41it/s]


 98%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊    | 1004/1026 [00:03<00:00, 291.65it/s]












































 25%|█████████████████████████████████████████████████▎                                                                                                                                                 | 465/1840 [09:12<21:17,  1.08it/s]Traceback (most recent call last):
  File "/home/annt/kbqa/multitask_mrc/multitask/train_qa.py", line 131, in <module>
    main(args)
  File "/home/annt/kbqa/multitask_mrc/multitask/train_qa.py", line 116, in main
    retro_reader.train()
  File "/home/annt/kbqa/multitask_mrc/multitask/src/qa.py", line 505, in train
    self.mrc_cls.train()
  File "/home/annt/miniconda3/lib/python3.9/site-packages/transformers/trainer.py", line 1662, in train
    return inner_training_loop(
  File "/home/annt/miniconda3/lib/python3.9/site-packages/transformers/trainer.py", line 1996, in _inner_training_loop
    self.optimizer.step()
  File "/home/annt/miniconda3/lib/python3.9/site-packages/torch/optim/lr_scheduler.py", line 65, in wrapper
    return wrapped(*args, **kwargs)
  File "/home/annt/miniconda3/lib/python3.9/site-packages/torch/optim/optimizer.py", line 113, in wrapper
    return func(*args, **kwargs)
  File "/home/annt/miniconda3/lib/python3.9/site-packages/transformers/optimization.py", line 445, in step
    exp_avg.mul_(beta1).add_(grad, alpha=(1.0 - beta1))
KeyboardInterrupt