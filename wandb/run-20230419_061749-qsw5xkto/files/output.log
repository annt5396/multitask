
  0%|                                                                                                                                                                                                             | 0/1840 [00:00<?, ?it/s]You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.










































































































































































 20%|███████████████████████████████████████                                                                                                                                                            | 368/1840 [05:44<20:10,  1.22it/s]































100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 257/257 [01:04<00:00,  4.77it/s]


 79%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                        | 814/1026 [00:02<00:00, 358.79it/s]


























































 27%|████████████████████████████████████████████████████▎                                                                                                                                              | 494/1840 [09:06<20:54,  1.07it/s]Traceback (most recent call last):
  File "/home/annt/kbqa/multitask_mrc/multitask/train_qa.py", line 131, in <module>
    main(args)
  File "/home/annt/kbqa/multitask_mrc/multitask/train_qa.py", line 116, in main
    retro_reader.train()
  File "/home/annt/kbqa/multitask_mrc/multitask/src/qa.py", line 504, in train
  File "/home/annt/miniconda3/lib/python3.9/site-packages/transformers/trainer.py", line 1662, in train
    return inner_training_loop(
  File "/home/annt/miniconda3/lib/python3.9/site-packages/transformers/trainer.py", line 1929, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/home/annt/miniconda3/lib/python3.9/site-packages/transformers/trainer.py", line 2717, in training_step
    loss.backward()
  File "/home/annt/miniconda3/lib/python3.9/site-packages/torch/_tensor.py", line 396, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/annt/miniconda3/lib/python3.9/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt