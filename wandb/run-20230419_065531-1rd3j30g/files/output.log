
  0%|                                                                                                                                                                                                             | 0/1840 [00:00<?, ?it/s]You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.









  1%|██▏                                                                                                                                                                                                 | 21/1840 [00:21<27:52,  1.09it/s]



  2%|███▎                                                                                                                                                                                                | 31/1840 [00:30<27:38,  1.09it/s]
{'loss': 3.2618, 'learning_rate': 4.347826086956522e-06, 'epoch': 0.11}

  7%|█████████████▍                                                                                                                                                                                     | 127/1840 [01:59<26:35,  1.07it/s]
{'loss': 2.5081, 'learning_rate': 8.695652173913044e-06, 'epoch': 0.22}
{'loss': 2.2437, 'learning_rate': 1.0869565217391305e-05, 'epoch': 0.27}






  8%|██████████████▊                                                                                                                                                                                    | 140/1840 [02:11<26:16,  1.08it/s]









  9%|████████████████▊                                                                                                                                                                                  | 159/1840 [02:29<25:55,  1.08it/s]










 10%|███████████████████▏                                                                                                                                                                               | 181/1840 [02:49<25:34,  1.08it/s]









 11%|█████████████████████▎                                                                                                                                                                             | 201/1840 [03:08<25:24,  1.07it/s]









 12%|███████████████████████▎                                                                                                                                                                           | 220/1840 [03:25<24:59,  1.08it/s]








 13%|█████████████████████████▍                                                                                                                                                                         | 240/1840 [03:44<24:41,  1.08it/s]
 13%|█████████████████████████▌                                                                                                                                                                         | 241/1840 [03:45<24:29,  1.09it/s]Traceback (most recent call last):
  File "/home/annt/kbqa/multitask_mrc/multitask/train_qa.py", line 131, in <module>
    main(args)
  File "/home/annt/kbqa/multitask_mrc/multitask/train_qa.py", line 116, in main
    retro_reader.train()
  File "/home/annt/kbqa/multitask_mrc/multitask/src/qa.py", line 505, in train
    self.mrc_cls.train()
  File "/home/annt/miniconda3/lib/python3.9/site-packages/transformers/trainer.py", line 1662, in train
    return inner_training_loop(
  File "/home/annt/miniconda3/lib/python3.9/site-packages/transformers/trainer.py", line 1931, in _inner_training_loop
    if (
KeyboardInterrupt